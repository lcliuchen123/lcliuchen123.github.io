<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>中文句向量研究 | Chen Liu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="毕业设计中文文本相似度–句向量整体实验设计(代码)一、文本相似度的三种常用方法&amp;#160; &amp;#160; &amp;#160; &amp;#160;本文主要利用余弦相似度衡量句子之间的相似度，因此如何生成优质的句向量至关重要。    相似度 具体做法    Jaccard系数 分词未分词   编辑距离 最小操作次数   余弦相似度 Word2vec+词+加权平均Word2vec+字+加权平均Word2vec+词+">
<meta property="og:type" content="article">
<meta property="og:title" content="中文句向量研究">
<meta property="og:url" content="http://yoursite.com/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/index.html">
<meta property="og:site_name" content="Chen Liu">
<meta property="og:description" content="毕业设计中文文本相似度–句向量整体实验设计(代码)一、文本相似度的三种常用方法&amp;#160; &amp;#160; &amp;#160; &amp;#160;本文主要利用余弦相似度衡量句子之间的相似度，因此如何生成优质的句向量至关重要。    相似度 具体做法    Jaccard系数 分词未分词   编辑距离 最小操作次数   余弦相似度 Word2vec+词+加权平均Word2vec+字+加权平均Word2vec+词+">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-04-30T09:56:02.000Z">
<meta property="article:modified_time" content="2020-04-30T10:03:24.184Z">
<meta property="article:author" content="Chen Liu">
<meta property="article:tag" content="句向量在文本相似度上的应用研究">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Chen Liu" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen Liu</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blogs</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-中文句向量研究" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/" class="article-date">
  <time datetime="2020-04-30T09:56:02.000Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      中文句向量研究
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="毕业设计"><a href="#毕业设计" class="headerlink" title="毕业设计"></a>毕业设计</h1><h2 id="中文文本相似度–句向量"><a href="#中文文本相似度–句向量" class="headerlink" title="中文文本相似度–句向量"></a>中文文本相似度–句向量</h2><h2 id="整体实验设计-代码"><a href="#整体实验设计-代码" class="headerlink" title="整体实验设计(代码)"></a>整体实验设计(<a href="https://github.com/lcliuchen123/sentence-embedding" target="_blank" rel="noopener">代码</a>)</h2><h3 id="一、文本相似度的三种常用方法"><a href="#一、文本相似度的三种常用方法" class="headerlink" title="一、文本相似度的三种常用方法"></a>一、文本相似度的三种常用方法</h3><p>&#160; &#160; &#160; &#160;本文主要利用余弦相似度衡量句子之间的相似度，<br>因此如何生成优质的句向量至关重要。</p>
<table>
<thead>
<tr>
<th align="center">相似度</th>
<th align="center">具体做法</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Jaccard系数</td>
<td align="center">分词<br>未分词</td>
</tr>
<tr>
<td align="center">编辑距离</td>
<td align="center">最小操作次数</td>
</tr>
<tr>
<td align="center">余弦相似度</td>
<td align="center">Word2vec+词+加权平均<br>Word2vec+字+加权平均<br>Word2vec+词+Qucik-thoughts<br>Word2vec+字+Quick-thoughts<br>字+融合Transformer的Quick-thoughts<br>词+融合Transformer的Quick-thoughts</td>
</tr>
</tbody></table>
<h3 id="二、实验步骤"><a href="#二、实验步骤" class="headerlink" title="二、实验步骤"></a>二、实验步骤</h3><h4 id="第一步：数据标注"><a href="#第一步：数据标注" class="headerlink" title="第一步：数据标注"></a>第一步：数据标注</h4><ol>
<li><p>计划标注：</p>
<ul>
<li>人工标注验证集（大约4000条数据，需要20天左右，10月25日之前完成。）</li>
<li>思路：133个类别，每个类别标注30条数据。</li>
<li>方法：利用关键词筛选出每个类别对应的数据。</li>
</ul>
</li>
<li><p>实际标注：</p>
<ul>
<li>实际只标注了2000条数据</li>
<li>1500条有对应的类别，500条没有作为反例，共涉及80个类别。</li>
</ul>
</li>
</ol>
<h4 id="第二步：训练词向量"><a href="#第二步：训练词向量" class="headerlink" title="第二步：训练词向量"></a>第二步：训练词向量</h4><ul>
<li>方法：Word2vec</li>
<li>具体做法：<ol>
<li>下载最新的中文维基百科数据集（1.5g)</li>
<li>进行一系列预处理操作：<ul>
<li><a href="https://github.com/attardi/wikiextractor" target="_blank" rel="noopener">wikiextractor</a>解压，提取压缩包文本信息 wiki_00</li>
<li>繁转简：opencc-&gt; wiki.zh.txt</li>
<li>分句+数据清洗（只保留数字、英文和汉字）：fen_ju.py-&gt; new_sentence.txt</li>
<li>是否分词: jieba </li>
<li>是否去停词：停词表</li>
<li>统计句子长度、词频、字频，生成字典</li>
</ul>
</li>
<li>分词后训练word2vec模型,生成2种词向量。<ul>
<li>分词+去停词+300维</li>
<li>未分词+去停词+300维</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="第三步：无监督学习的句向量生成算法"><a href="#第三步：无监督学习的句向量生成算法" class="headerlink" title="第三步：无监督学习的句向量生成算法"></a>第三步：无监督学习的句向量生成算法</h4><ol>
<li><p>在文本相似度任务中比较各种句向量生成算法的效果</p>
<ol>
<li>加权平均算法</li>
<li>Quick-Thoughts算法</li>
<li>将Transformer融入Quick-Thought算法中，观察其效果</li>
<li>比较分词与未分词的差别</li>
</ol>
</li>
<li><p>具体做法</p>
<ul>
<li>实际数据共有1982条进行预测，预定义语句5166条，因为部分数据预处理后为空</li>
<li>本文修改了f1的计算方式—因为数据中存在没有对应的预定义操作的句子</li>
<li>对于深度学习模型，本文的实验环境有限，只选择了80万条训练样本</li>
</ul>
<ol>
<li><p>方法一：基于词向量的简单平均算法. 共耗时1771.08s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.6846</td>
<td align="center">0.7070</td>
<td align="center">0.6956</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.6868</td>
<td align="center">0.7070</td>
<td align="center">0.6968</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.6931</td>
<td align="center">0.7070</td>
<td align="center">0.7000</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.8070</td>
<td align="center">0.7050</td>
<td align="center">0.7526</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.9525</td>
<td align="center">0.6167</td>
<td align="center">0.7487</td>
</tr>
</tbody></table>
</li>
<li><p>方法二：基于字向量的简单平均算法，共耗时1762.93s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.6970</td>
<td align="center">0.7492</td>
<td align="center">0.7221</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.6996</td>
<td align="center">0.7492</td>
<td align="center">0.7235</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.7703</td>
<td align="center">0.7492</td>
<td align="center">0.7596</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.8749</td>
<td align="center">0.7391</td>
<td align="center">0.8013</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.9607</td>
<td align="center">0.5719</td>
<td align="center">0.7170</td>
</tr>
</tbody></table>
</li>
<li><p>方法三：基于词向量的加权平均算法. 共耗时1834.97s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.687219</td>
<td align="center">0.715719</td>
<td align="center">0.701180</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.687219</td>
<td align="center">0.715719</td>
<td align="center">0.701180</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.701180</td>
<td align="center">0.715719</td>
<td align="center">0.708375</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.838095</td>
<td align="center">0.706355</td>
<td align="center">0.766606</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.923699</td>
<td align="center">0.534448</td>
<td align="center">0.677119</td>
</tr>
</tbody></table>
</li>
<li><p>方法四：基于字向量的加权平均算法. 共耗时1787.23s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.692551</td>
<td align="center">0.733779</td>
<td align="center">0.712569</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.692988</td>
<td align="center">0.733779</td>
<td align="center">0.712801</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.754301</td>
<td align="center">0.733110</td>
<td align="center">0.743555</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.879508</td>
<td align="center">0.717726</td>
<td align="center">0.790424</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.949233</td>
<td align="center">0.537793</td>
<td align="center">0.686593</td>
</tr>
</tbody></table>
</li>
<li><p>方法五：基于字向量和词向量的加权平均算法. 共耗时1762.93s.<br><br>生成的句向量是600维</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.697328</td>
<td align="center">0.750502</td>
<td align="center">0.722938</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.698195</td>
<td align="center">0.750502</td>
<td align="center">0.723404</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.763624</td>
<td align="center">0.749833</td>
<td align="center">0.756666</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.890433</td>
<td align="center">0.728428</td>
<td align="center">0.801325</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.950423</td>
<td align="center">0.525753</td>
<td align="center">0.677003</td>
</tr>
</tbody></table>
</li>
<li><p>方法六：基于词向量的Quick_Thoughts算法</p>
<ol>
<li><p>生成训练样本Tfrecords文件—preprocess_dataset.py</p>
</li>
<li><p>训练模型—train.py</p>
<table>
<thead>
<tr>
<th align="center">需要调节的参数</th>
<th align="center">解释说明</th>
<th align="center">示例</th>
</tr>
</thead>
<tbody><tr>
<td align="center">word2vec_path</td>
<td align="center">word2vec文件的目录</td>
<td align="center">../data/sent_word_n/</td>
</tr>
<tr>
<td align="center">output_dir</td>
<td align="center">生成句向量的目录</td>
<td align="center">../output/sent_char_n/</td>
</tr>
<tr>
<td align="center">input_file_pattern</td>
<td align="center">tfrecord文件的命名格式</td>
<td align="center">../output/sent_word_n/train-?????-of-00010</td>
</tr>
<tr>
<td align="center">train_dir</td>
<td align="center">模型文件的保存位置</td>
<td align="center">..model/train/sent_char_n</td>
</tr>
</tbody></table>
</li>
<li><p>预测：生成句向量—predict.py</p>
</li>
<li><p>训练结果</p>
<ul>
<li><p>生成句向量消耗的时间：14.37s（小爱数据，1982条），24.17s（预定义数据集，5166条）</p>
</li>
<li><p>整个预测过程共消耗21950.48s</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.655587</td>
<td align="center">0.620067</td>
<td align="center">0.637332</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.655587</td>
<td align="center">0.620067</td>
<td align="center">0.637332</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.655587</td>
<td align="center">0.620067</td>
<td align="center">0.637332</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.661670</td>
<td align="center">0.620067</td>
<td align="center">0.640193</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.717054</td>
<td align="center">0.618729</td>
<td align="center">0.664273</td>
</tr>
<tr>
<td align="center">0.94</td>
<td align="center">0.847909</td>
<td align="center">0.596656</td>
<td align="center">0.700432</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
</li>
<li><p>方法七：基于字向量的Quick-Thoughts算法</p>
<ul>
<li><p>训练结果：</p>
<ul>
<li><p>生成句向量消耗的时间：<br>   13.72s（小爱数据，1982条），23.98s（预定义数据集，5166条）</p>
</li>
<li><p>共消耗21926.511472 s</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.673374</td>
<td align="center">0.671572</td>
<td align="center">0.672472</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.673374</td>
<td align="center">0.671572</td>
<td align="center">0.672472</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.673826</td>
<td align="center">0.671572</td>
<td align="center">0.672697</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.679756</td>
<td align="center">0.671572</td>
<td align="center">0.675639</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.839361</td>
<td align="center">0.667559</td>
<td align="center">0.743666</td>
</tr>
<tr>
<td align="center">0.92</td>
<td align="center">0.900742</td>
<td align="center">0.649498</td>
<td align="center">0.754761</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法八：融合Transformer的Quick-Thoughts算法（分词, 2700维）</p>
<ol>
<li><p>Transform的编码器得到的是一个[seq_length, dim]的向量，<br>因此探索Transformer编码器生成句向量的处理方式，然后与Quick-Thoughts融合</p>
<ul>
<li><p>生成句向量消耗的时间：26.67s（小爱数据，1982条），40.14s（预定义数据集，5166条）</p>
</li>
<li><p>模型参数设置（调整后得到的最优参数）：</p>
<table>
<thead>
<tr>
<th align="center">参数</th>
<th align="center">取值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">num_head</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">learing_rate</td>
<td align="center">0.001</td>
</tr>
<tr>
<td align="center">dim</td>
<td align="center">2700</td>
</tr>
<tr>
<td align="center">dropout_rate</td>
<td align="center">0.3</td>
</tr>
<tr>
<td align="center">batch_zie</td>
<td align="center">128</td>
</tr>
<tr>
<td align="center">num_ceng</td>
<td align="center">2</td>
</tr>
</tbody></table>
</li>
<li><p>预测结果对比：</p>
<table>
<thead>
<tr>
<th align="center">处理方式</th>
<th align="center">F1值（最优）</th>
<th align="center">预测消耗总时间（s）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">简单平均</td>
<td align="center">0.63</td>
<td align="center">26122.49</td>
</tr>
<tr>
<td align="center">直接求和</td>
<td align="center">0.33</td>
<td align="center">24774.29</td>
</tr>
<tr>
<td align="center">标准化后平均</td>
<td align="center">0.40</td>
<td align="center">24528.78</td>
</tr>
<tr>
<td align="center">标准化后求和</td>
<td align="center">0.39</td>
<td align="center">27671.44</td>
</tr>
<tr>
<td align="center">对简单平均后的向量进行标准化</td>
<td align="center">0.45</td>
<td align="center">24164.85</td>
</tr>
<tr>
<td align="center">对直接求和后的向量进行标准化</td>
<td align="center">0.1</td>
<td align="center">21877.37</td>
</tr>
</tbody></table>
<p>最优的F1值为0.69，阈值为0.999588。</p>
</li>
</ul>
<p><strong>结论：对于Transformer模型生成的词向量进行简单平均效果最好</strong></p>
</li>
</ol>
</li>
<li><p>方法九：融合Transformer的Quick-Thoughts算法（未分词，2700维）</p>
<ul>
<li><p>基于字向量的Transformer模型：</p>
<ol>
<li>损失函数值：1102.33</li>
<li>训练时间： 48704.05</li>
</ol>
</li>
<li><p><em>结果：最优的F1值为0.75，阈值为0.999442。*</em></p>
</li>
</ul>
</li>
<li><p>方法十：Transformer编码器+简单平均</p>
<table>
<thead>
<tr>
<th align="center">模型</th>
<th align="center">阈值</th>
<th align="center">最优的F1值</th>
<th align="center">预测时间</th>
<th align="center">日志文件名</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于字向量的Transformer模型</td>
<td align="center">0.999991</td>
<td align="center">0.41</td>
<td align="center">8686.1</td>
<td align="center">tr_char</td>
</tr>
<tr>
<td align="center">基于词向量的Transformer模型</td>
<td align="center">0.999999</td>
<td align="center">0.30</td>
<td align="center">9753.86</td>
<td align="center">tr_word</td>
</tr>
</tbody></table>
<p> <strong>结论：仅利用Transformer编码器无法揭示句子之间的相似程度。</strong></p>
</li>
</ol>
</li>
</ol>
<h4 id="第四步：训练细节"><a href="#第四步：训练细节" class="headerlink" title="第四步：训练细节"></a>第四步：训练细节</h4><p>  <strong>利用sent_word_rem数据集对Transformer进行调参</strong></p>
<ol>
<li><p><strong>第一次调参时模型存在部分错误：</strong></p>
<ol>
<li>mask应该相乘而不是相加</li>
<li>多头注意力的输出应该添加一个线性连接层</li>
<li>多头注意力层和全连接层没有添加drpout</li>
<li>词向量矩阵应该随机正态化，没有对词向量矩阵和切分后的q进行归一化</li>
</ol>
</li>
<li><p>第一调参得到的部分错误结果    </p>
<ul>
<li><p>（1）观察预先训练的词向量对模型结果的影响</p>
<table>
<thead>
<tr>
<th align="center">词向量表示方式</th>
<th align="center">损失函数值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">预先训练</td>
<td align="center">1257.96</td>
</tr>
<tr>
<td align="center">随机初始化</td>
<td align="center">1224.76</td>
</tr>
</tbody></table>
</li>
<li><p>（2）编码器和解码器层数对模型结果的影响</p>
<table>
<thead>
<tr>
<th align="center">层数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center"></td>
<td align="center">batch_size=128</td>
</tr>
</tbody></table>
</li>
<li><p>（3）编码维度对模型的结果影响</p>
<table>
<thead>
<tr>
<th align="center">维度</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">300</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
</tbody></table>
</li>
<li><p>（4）头数对模型结果的影响(双层，batch_size=128)</p>
<table>
<thead>
<tr>
<th align="center">头数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1225.45</td>
<td align="center">67956.94</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1225.45</td>
<td align="center">68619.52</td>
</tr>
</tbody></table>
</li>
<li><p>（5）batch_size对模型的影响(双层，num_head=4))</p>
<table>
<thead>
<tr>
<th align="center">batch_size</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">128</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
<tr>
<td align="center">64</td>
<td align="center">608.16</td>
<td align="center">68647.59</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p><strong>第二次调参</strong></p>
<ul>
<li><p>batch_size =128, ceng_shu=2, num_head=6</p>
</li>
<li><p>（1）batch_size对模型的影响(双层，num_head=4))</p>
<table>
<thead>
<tr>
<th align="center">batch_size</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">512</td>
<td align="center">OOM</td>
<td align="center">OOM</td>
</tr>
<tr>
<td align="center">256</td>
<td align="center">2457.10</td>
<td align="center">70999.85</td>
</tr>
<tr>
<td align="center">128</td>
<td align="center">1242.77</td>
<td align="center">71327.49</td>
</tr>
<tr>
<td align="center">64</td>
<td align="center">623.93</td>
<td align="center">71453.79</td>
</tr>
</tbody></table>
<p> <strong>问题</strong></p>
<pre><code>1. 为什么batch_size一般选择2的幂次？
   &lt;br&gt;因为GPU对2的幂次的batch可以发挥更佳的性能
2. batch_size对模型效果的影响？
   （1）batch_size过大，训练消耗的时间会缩短，但是模型容易陷入局部最优点。
      因为样本方差较小，可能会呆在一个局部最优点不动。
   （2）batch_size过小，模型同样不易收敛，损失函数容易震荡</code></pre></li>
<li><p>（2）编码器和解码器层数对模型结果的影响（batch_size=128,num_head=2)</p>
<table>
<thead>
<tr>
<th align="center">层数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1143.48</td>
<td align="center">71261.14</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">1222.36</td>
<td align="center">83205.03</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1224.50</td>
<td align="center">94817.31</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">1225.24</td>
<td align="center">106313.79</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1229.86</td>
<td align="center">117888.51</td>
</tr>
</tbody></table>
</li>
<li><p>（3）编码维度对模型的结果影响</p>
<pre><code>batch_size=128,num_head=8,
num_units=[512,2048],num_ceng=2
dim = 200时，训练结果不稳定，参考loss.png(16,17)
dim = 300时，num_head = 2</code></pre><table>
<thead>
<tr>
<th align="center">维度</th>
<th align="center">损失函数值</th>
<th align="left">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">200</td>
<td align="center">1143.52</td>
<td align="left">53102.00</td>
</tr>
<tr>
<td align="center">256</td>
<td align="center">1143.49</td>
<td align="left">63260.60</td>
</tr>
<tr>
<td align="center">300</td>
<td align="center">1143.48</td>
<td align="left">71201.66</td>
</tr>
<tr>
<td align="center">400</td>
<td align="center">1143.48</td>
<td align="left">94607.44</td>
</tr>
<tr>
<td align="center">512</td>
<td align="center">1143.47</td>
<td align="left">123626.50</td>
</tr>
</tbody></table>
</li>
<li><p>（4）头数对模型结果的影响(双层，batch_size=128)</p>
<table>
<thead>
<tr>
<th align="center">头数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1143.48</td>
<td align="center">71261.14</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1227.60</td>
<td align="center">71728.84</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">1143.78</td>
<td align="center">71856.85</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1143.48</td>
<td align="center">71636.31</td>
</tr>
<tr>
<td align="center">10</td>
<td align="center">1143.48</td>
<td align="center">73083.46</td>
</tr>
<tr>
<td align="center">20</td>
<td align="center">1143.48</td>
<td align="center">74627.01</td>
</tr>
</tbody></table>
<p><strong>问题</strong>  </p>
<pre><code>1. 为什么头数为4的时候损失函数值比较高？</code></pre></li>
<li><p>（5）学习率对模型结果的影响（双层, num_head = 6, batch_size=128)</p>
<table>
<thead>
<tr>
<th align="center">rate</th>
<th align="center">损失函数值</th>
<th align="center">F1值（最优）</th>
<th align="center">阈值</th>
<th align="center">训练模型消耗时间（s)</th>
<th align="center">预测消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.00005</td>
<td align="center">1235.80</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">120375.26</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.000275</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.0005</td>
<td align="center">1143.48</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">71636.31</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.001</td>
<td align="center">1143.48</td>
<td align="center">0.69</td>
<td align="center">0.999588</td>
<td align="center">62813.25</td>
<td align="center">9296.24</td>
</tr>
<tr>
<td align="center">0.003</td>
<td align="center">1143.47</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">72178.98</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.005</td>
<td align="center">1143.47</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">71888.02</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.01</td>
<td align="center">1143.80</td>
<td align="center">0.69</td>
<td align="center">0.989927</td>
<td align="center">67776.89</td>
<td align="center">9477.49</td>
</tr>
</tbody></table>
</li>
<li><p>（6）探索直接输入预训练的词向量还是随机初始化效果较好<br>   （num_head =2, dim=300, num_units=[1200, 300], loss_19.png)</p>
<ul>
<li><p>开始主观设定阈值的取值范围是0.5：1：0.01</p>
<table>
<thead>
<tr>
<th align="center">是否采用预训练的词向量作为输入</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">是</td>
<td align="center">1143.51</td>
<td align="center">62143.24</td>
</tr>
<tr>
<td align="center">否</td>
<td align="center">1143.48</td>
<td align="center">71201.66</td>
</tr>
</tbody></table>
</li>
<li><p>然后调整阈值取值范围为最小值：最大值：（最大值-最小值）/20</p>
<table>
<thead>
<tr>
<th align="center">是否采用预训练的词向量作为输入</th>
<th align="center">F1值（最优）</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">是</td>
<td align="center">0.63</td>
<td align="center">26122.49</td>
</tr>
<tr>
<td align="center">否</td>
<td align="center">0.63</td>
<td align="center">21611.44</td>
</tr>
</tbody></table>
</li>
</ul>
<p><strong>结论</strong>：利用预训练得到的词向量得到的输入，模型训练过程中会发生震荡，<br>  但是最终的结果与随机初始化相差不大，而且利用预先训练的词向量训练模型消耗的时间较少。</p>
</li>
<li><p>(7) 探索dropout的影响（随机初始化词向量）</p>
<pre><code>dim =300, num_head =2, num_units = [1200, 300],
num_ceng =2, batch_size = 128, 
learning_rate = 0.0005
阈值是用（最大值-最小值）/20为步长挑选出来的</code></pre><table>
<thead>
<tr>
<th align="center">rate</th>
<th align="center">损失函数值</th>
<th align="center">F1值（最优）</th>
<th align="center">阈值</th>
<th align="center">训练模型消耗时间（s)</th>
<th align="center">预测消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.1</td>
<td align="center">1143.47</td>
<td align="center">0.69</td>
<td align="center">0.999607</td>
<td align="center">62943.26</td>
<td align="center">9107.41</td>
</tr>
<tr>
<td align="center">0.3</td>
<td align="center">1143.48</td>
<td align="center">0.69</td>
<td align="center">0.999605</td>
<td align="center">62623.19</td>
<td align="center">9271.86</td>
</tr>
<tr>
<td align="center">0.5</td>
<td align="center">1143.58</td>
<td align="center">0.69</td>
<td align="center">0.999606</td>
<td align="center">62186.54</td>
<td align="center">9137.37</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">1144.84</td>
<td align="center">0.69</td>
<td align="center">0.999590</td>
<td align="center">63128.25</td>
<td align="center">9588.84</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<p>  <strong>调参结果</strong></p>
<pre><code>1. dim =300, num_head =2, num_units = [1200, 300],
   num_ceng =2, batch_size = 128, rate=0.3
2. learning_rate = 0.0005, 梯度更新公式在论文的基础上乘以learning_rate效果更好
3. 是否采用预训练的词向量对损失函数值影响不大（已有论文证明，而且本文的结果也证明是否采用预训练的词向量影响不大），但是采用预训练的词向量消耗的时间较短。
   在预测时对比两种模型在真是数据集中的效果，发现效果差别不大。
4. 受硬件限制无法训练一个与原论文相同的6层512维的模型，大概需要三天左右才能训练结束。</code></pre><h4 id="第五步：实验结果"><a href="#第五步：实验结果" class="headerlink" title="第五步：实验结果"></a>第五步：实验结果</h4><ol>
<li><p>实验环境:<strong>2核8g服务器</strong></p>
</li>
<li><p>生成处理后的数据集需要的时间</p>
<table>
<thead>
<tr>
<th align="center">数据集</th>
<th align="center">生成字向量或者词向量(s)</th>
<th align="center">获取词频文件(s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sent_char_n.txt</td>
<td align="center">4065.48</td>
<td align="center">445</td>
</tr>
<tr>
<td align="center">sent_char_rem.txt</td>
<td align="center">2663.45</td>
<td align="center">334</td>
</tr>
<tr>
<td align="center">sent_word_n.txt</td>
<td align="center">4594.10</td>
<td align="center">443</td>
</tr>
<tr>
<td align="center">sent_word_rem.txt</td>
<td align="center">3359.56</td>
<td align="center">366</td>
</tr>
</tbody></table>
</li>
<li><p>Quick-thoughts训练过程统计</p>
<ul>
<li><p>第一次生成训练数据的时间</p>
<table>
<thead>
<tr>
<th align="center">数据集</th>
<th align="center">词或字的总数量</th>
<th align="center">词向量中字或词的数量</th>
<th align="center">统计词频(s)</th>
<th align="center">生成词典和词向量文件(s)</th>
<th align="center">样本中句子包含<br>最少的词数量</th>
<th align="center">样本中句子<br>包含最多的词数量</th>
<th align="center">训练时的词数量</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sent_char_n.txt</td>
<td align="center">19354</td>
<td align="center">12820</td>
<td align="center">111.06</td>
<td align="center">10.43</td>
<td align="center">2</td>
<td align="center">874</td>
<td align="center">12820</td>
</tr>
<tr>
<td align="center">sent_char_rem.txt</td>
<td align="center">18936</td>
<td align="center">12403</td>
<td align="center">78.99</td>
<td align="center">10.28</td>
<td align="center">2</td>
<td align="center">767</td>
<td align="center">12403</td>
</tr>
<tr>
<td align="center">sent_word_n.txt</td>
<td align="center">3198221</td>
<td align="center">617297</td>
<td align="center">117.14</td>
<td align="center">140.89</td>
<td align="center">2</td>
<td align="center">432</td>
<td align="center">20000</td>
</tr>
<tr>
<td align="center">sent_word_rem.txt</td>
<td align="center">3196599</td>
<td align="center">615709</td>
<td align="center">92.02</td>
<td align="center">141.54</td>
<td align="center">2</td>
<td align="center">366</td>
<td align="center">20000</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">数据集</th>
<th align="center">生成TF文件耗费时间（s)</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sent_char_n.txt</td>
<td align="center">86.21</td>
<td align="center">29555.58</td>
</tr>
<tr>
<td align="center">sent_char_rem.txt</td>
<td align="center">76.69</td>
<td align="center">29743.32</td>
</tr>
<tr>
<td align="center">sent_word_n.txt</td>
<td align="center">70.86</td>
<td align="center">29528.91</td>
</tr>
<tr>
<td align="center">sent_word_rem.txt</td>
<td align="center">80.22</td>
<td align="center">29533.81</td>
</tr>
</tbody></table>
</li>
<li><p>第二次重新分句后生成训练数据的时间</p>
<table>
<thead>
<tr>
<th align="center">文件</th>
<th align="center">字或词的数目</th>
<th align="center">训练样本中长度<br>小于30的句子数量</th>
<th align="center">句子的最短长度</th>
<th align="center">句子的最大长度</th>
<th align="center">生成record文件<br>消耗的时间(s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">new_sent_char_rem.txt</td>
<td align="center">9591</td>
<td align="center">640195</td>
<td align="center">2</td>
<td align="center">997</td>
<td align="center">78.80</td>
</tr>
<tr>
<td align="center">new_sent_word_rem.txt</td>
<td align="center">20000</td>
<td align="center">938007</td>
<td align="center">2</td>
<td align="center">300</td>
<td align="center">72.19</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>第一次训练的最终结果：</p>
<ul>
<li><p>第一次利用Quick-Thoughts预测时，阈值是固定的（0.9)。</p>
</li>
<li><p>第一次利用融合模型预测的时候，阈值是从0.5~1，步长为0.01，所以预测时间较长。</p>
</li>
<li><p>第一次利用融合模型预测时，基于词向量的融合模型效果较差，因此没有预测基于字向量的模型。</p>
</li>
<li><p>第一次训练时部分日志文件丢失，无法获取准确的训练时间。</p>
<table>
<thead>
<tr>
<th align="center">模型</th>
<th align="center">阈值</th>
<th align="center">最优的F1值</th>
<th align="center">预测时间</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于词向量的简单平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.7526</td>
<td align="center">1771.08</td>
</tr>
<tr>
<td align="center">基于字向量的简单平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.8013</td>
<td align="center">1762.93</td>
</tr>
<tr>
<td align="center">基于词向量的加权平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.7666</td>
<td align="center">1834.97</td>
</tr>
<tr>
<td align="center">基于字向量的加权平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.7904</td>
<td align="center">1787.23</td>
</tr>
<tr>
<td align="center">基于字向量和词向量的加权平均</td>
<td align="center">0.8</td>
<td align="center">0.8013</td>
<td align="center">1762.93</td>
</tr>
<tr>
<td align="center">基于词向量的Quick-Thought Vectors算法</td>
<td align="center">0.9</td>
<td align="center">0.743666</td>
<td align="center">2253.14</td>
</tr>
<tr>
<td align="center">基于字向量的Quick-Thought Vectors算法</td>
<td align="center">0.9</td>
<td align="center">0.664273</td>
<td align="center">2250.50</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought Vectors算法（分词+去停词）</td>
<td align="center">0.8</td>
<td align="center">0.396015</td>
<td align="center">24528.78</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought Vectors算法（不分词+去停词）</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>第二次重新分句后的最终结果</p>
<ul>
<li><p>对于简单平均算法和加权平均算法没有重新生成词向量，进行训练。</p>
</li>
<li><p>重新对数据进行清洗，删除维基百科中部分无意义的信息，<br>并且按照中文习惯重新进行分句。</p>
</li>
<li><p>训练模型模型时，全都随机初始化词向量。（所以Quick-thoughts效果较差）</p>
<p><strong>实验结果</strong></p>
<table>
<thead>
<tr>
<th align="center">模型</th>
<th align="center">阈值</th>
<th align="center">最优的F1值</th>
<th align="center">训练时间（s)</th>
<th align="center">预测时间(s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于字向量的Quick-Thoughts模型</td>
<td align="center">0.922823</td>
<td align="center">0.601799</td>
<td align="center">31274.17(8.7h)</td>
<td align="center">9354.07</td>
</tr>
<tr>
<td align="center">基于词向量的Quick-Thoughts模型</td>
<td align="center">0.885814</td>
<td align="center">0.647385</td>
<td align="center">31443.50(8.7h)</td>
<td align="center">9192.68</td>
</tr>
<tr>
<td align="center">基于字向量的Transformer模型</td>
<td align="center">0.999912</td>
<td align="center">0.725537</td>
<td align="center">47057.17(13h)</td>
<td align="center">8854.41</td>
</tr>
<tr>
<td align="center">基于词向量的Transformer模型</td>
<td align="center">0.999942</td>
<td align="center">0.575241</td>
<td align="center">72569.82(20h)</td>
<td align="center">8830.47</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought模型（未分词）</td>
<td align="center">0.999464</td>
<td align="center">0.63471</td>
<td align="center">31274.17(8.7h)+47057.17(13h)</td>
<td align="center">9397.37</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought模型（分词）</td>
<td align="center">0.999164</td>
<td align="center">0.64511</td>
<td align="center">31443.50(8.7h)+72569.82(20h)</td>
<td align="center">9328.97</td>
</tr>
</tbody></table>
</li>
<li><p>结论：</p>
<ol>
<li>两个编码器全都随机初始化词向量后，Quick-Thoughts的性能下降。<br>并且基于字向量的模型比基于词向量的模型效果较差。</li>
<li>仅用Transformer编码器，基于字向量的模型效果较好，基于词向量的模型效果特别差。</li>
<li>融合后的模型无论是基于词向量还是字向量，与融合前的Quick-thoughts算法相比，没有显著的提升。</li>
<li>融合前后的Quick-thoughts算法，基于字向量的模型效果都比基于词向量的模型效果差，可能是因为Quick-thoughts算法没有<br>输入预先训练的词向量，还有融合后句向量的维度变为2700，对模型的结果也有一定的影响，但是由于计算条件的限制无法对<br>句向量的维度进行探索。</li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="三、其它"><a href="#三、其它" class="headerlink" title="三、其它"></a>三、其它</h3><ul>
<li><p>已解决的问题</p>
<ol>
<li>词向量文件太大, 无法加载(已解决)—利用np.load加载.npy文件，直接就是numpy数组</li>
<li>为什么字输入比词输入消耗时间少？—原因：字的维度较小，权重矩阵较小。</li>
<li>换新电脑后经常出现无法打开GitHub的官网情况，原因为本地的DNS无法进行解析，<br>可以修改C:\Windows\System32\drivers\etc\hosts文件，具体细节参考<br><a href="https://blog.csdn.net/believe_s/article/details/81539747" target="_blank" rel="noopener">连不上GitHub的解决方案</a></li>
<li>label smoothing 标签平滑</li>
<li>batch_size固定的太死，无法预测非batch_size的东西</li>
<li>预测时的batch必须要与训练时的batch保持一样吗？—不一定</li>
<li>不足一个batch的数据在预测时是如何处理的？— tf.shape可以获取变量的维度信息，就算是维度为None</li>
<li>transformer的输入是等长的还是不等长的？<ul>
<li>编码器-解码器的每个batch的长度不一样，每个batch填充到这个batch内的最大长度。</li>
</ul>
</li>
<li>Quick-thoughts算法不需要对句子进行padding，transformer的每个batch需要进行padding</li>
<li>os.remove只能删除文件，shutil.rmtree可以删除指定的目录</li>
<li>第一次生成的vocab.txt中含有特殊字符，需要重新生成。<br>之前的vocab.txt是利用词向量文件重新生成的，所以要想生成新的必须首先生成词向量。<br>第二次修改直接选择词频较高的词语作为字典，总数量不超过20000。</li>
<li>重新生成训练样本文件，深度学习模型的词向量嵌入均随机初始化。</li>
</ol>
</li>
<li><p>未解决的问题</p>
<ol>
<li><p>长度和重合度数据集未标注完成，共2000条,并且标注后的样本去停词后会变成空值</p>
</li>
<li><p>受硬件限制，无法探索词向量的维度和句向量的维度对模型效果的影响</p>
</li>
<li><p>未登录词如何处理？<br>  <br>目前采取的方法：随机初始化，用0进行padding<br>  <br>（1）加unnk,索引为1<br>  <br>（2）随机初始化一个向量<br>  <br>（3）padding 和未登录词的区别</p>
</li>
<li><p>quick-thoughts 效果较差</p>
<ul>
<li>基于词向量的F1值0.7，基于字向量的F1值0.75.</li>
</ul>
</li>
<li><p>loss一直保持不变，是什么原因？</p>
<ul>
<li>猜测原因：<ol>
<li>说明参数一直都没有得到更新</li>
<li>没有添加dropout，为啥dropout可以防止过拟合？dropout相当于集成</li>
<li>batch_size过大，网络会收敛到局部最优点；<br>batchz_size太小，类别较多时，loss可能会一直震荡</li>
<li>学习率过大，transformer的学习率在论文中有对应的公式</li>
</ol>
</li>
<li>真实原因：<ol>
<li>按照论文中的学习率公式修改学习率后，损失函数缓慢下降，但是又开始震荡<ul>
<li>论文中的学习率公式如何得到的？？？</li>
<li>为什么按照论文中的公式表示学习率就会缓慢下降？？？？<br>理论上学习率如果足够小，肯定可以收敛。、</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>beam search在预测时会遇到，本文参考tensor2tensor的代码，并没有看懂    </p>
<ul>
<li>主要函数：<ol>
<li>grow_alive </li>
<li>grow_finished </li>
<li>grow_topk </li>
<li>inner_loop</li>
<li>is_not_finished</li>
</ol>
</li>
<li>不懂的点<ol>
<li>tf.bitcast()具体是怎末实现的</li>
<li>长度惩罚项及结束搜索的条件</li>
<li>主要函数的功能</li>
</ol>
</li>
</ul>
</li>
<li><p>机器翻译时，解码器的第一个输入是<code>&lt;s&gt;</code>表示开头，直到<code>&lt;e&gt;</code>结束</p>
</li>
<li><p>对词层面上的优化（感觉意义不大）：</p>
<ol>
<li>对句子中所有的词向量取最大值</li>
<li>对句子中所有的词向量取最小值</li>
<li>对句子中所有的词向量取平均值<br>生成句向量：<ol>
<li>进行拼接</li>
<li>删除掉最大值和最小值,然后再进行平均</li>
</ol>
</li>
</ol>
</li>
<li><p>第二次重新分句后没有训练词向量和字向量文件，quick-thoughts算法只能全部随机初始化，效果较差。</p>
</li>
</ol>
</li>
</ul>
<h3 id="四、-参考链接"><a href="#四、-参考链接" class="headerlink" title="四、 参考链接"></a>四、 参考链接</h3><ol>
<li><a href="https://blog.csdn.net/CiciliarCai/article/details/52948275" target="_blank" rel="noopener">维基百科</a></li>
<li><a href="https://github.com/PrincetonML/SIF" target="_blank" rel="noopener">加权平均算法</a></li>
<li><a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Transformer</a></li>
<li><a href="https://github.com/jinjiajia/Quick_Thought" target="_blank" rel="noopener">Quick-thoughts算法</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/" data-id="ck9mlp1t90004oot9b9ooakoj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8F%A5%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/" rel="tag">句向量在文本相似度上的应用研究</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/03/21/data-structure/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">data_structure</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/" rel="tag">git常用命令学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%A5%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/" rel="tag">句向量在文本相似度上的应用研究</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%EF%BC%9F-%E5%88%A9%E7%94%A8GitHub%E5%92%8CHexo%E6%9E%84%E5%BB%BA/" rel="tag">如何部署自己的博客系统？--利用GitHub和Hexo构建</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E6%89%A3%E5%88%B7%E9%A2%98%EF%BC%8C-%E4%BB%A3%E7%A0%81-https-gitee-com-chen-liu-123-project/" rel="tag">领扣刷题，[代码](https://gitee.com/chen_liu_123/project)</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">git常用命令学习</a> <a href="/tags/%E5%8F%A5%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/" style="font-size: 10px;">句向量在文本相似度上的应用研究</a> <a href="/tags/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%EF%BC%9F-%E5%88%A9%E7%94%A8GitHub%E5%92%8CHexo%E6%9E%84%E5%BB%BA/" style="font-size: 10px;">如何部署自己的博客系统？--利用GitHub和Hexo构建</a> <a href="/tags/%E9%A2%86%E6%89%A3%E5%88%B7%E9%A2%98%EF%BC%8C-%E4%BB%A3%E7%A0%81-https-gitee-com-chen-liu-123-project/" style="font-size: 10px;">领扣刷题，[代码](https://gitee.com/chen_liu_123/project)</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/">中文句向量研究</a>
          </li>
        
          <li>
            <a href="/2020/03/21/data-structure/">data_structure</a>
          </li>
        
          <li>
            <a href="/2020/03/10/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">git常用命令总结</a>
          </li>
        
          <li>
            <a href="/2020/03/09/first-blog/">first_blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Chen Liu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>