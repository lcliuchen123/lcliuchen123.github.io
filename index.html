<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Chen Liu</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="对自己的日常学习进行总结">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen Liu">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Chen Liu">
<meta property="og:description" content="对自己的日常学习进行总结">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Chen Liu">
<meta property="article:tag" content="自然语言处理">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Chen Liu" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen Liu</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">my blogs</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-中文句向量研究" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/" class="article-date">
  <time datetime="2020-04-30T09:56:02.000Z" itemprop="datePublished">2020-04-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/">中文句向量研究</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="毕业设计"><a href="#毕业设计" class="headerlink" title="毕业设计"></a>毕业设计</h1><h2 id="中文文本相似度–句向量"><a href="#中文文本相似度–句向量" class="headerlink" title="中文文本相似度–句向量"></a>中文文本相似度–句向量</h2><h2 id="整体实验设计-代码"><a href="#整体实验设计-代码" class="headerlink" title="整体实验设计(代码)"></a>整体实验设计(<a href="https://github.com/lcliuchen123/sentence-embedding" target="_blank" rel="noopener">代码</a>)</h2><h3 id="一、文本相似度的三种常用方法"><a href="#一、文本相似度的三种常用方法" class="headerlink" title="一、文本相似度的三种常用方法"></a>一、文本相似度的三种常用方法</h3><p>&#160; &#160; &#160; &#160;本文主要利用余弦相似度衡量句子之间的相似度，<br>因此如何生成优质的句向量至关重要。</p>
<table>
<thead>
<tr>
<th align="center">相似度</th>
<th align="center">具体做法</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Jaccard系数</td>
<td align="center">分词<br>未分词</td>
</tr>
<tr>
<td align="center">编辑距离</td>
<td align="center">最小操作次数</td>
</tr>
<tr>
<td align="center">余弦相似度</td>
<td align="center">Word2vec+词+加权平均<br>Word2vec+字+加权平均<br>Word2vec+词+Qucik-thoughts<br>Word2vec+字+Quick-thoughts<br>字+融合Transformer的Quick-thoughts<br>词+融合Transformer的Quick-thoughts</td>
</tr>
</tbody></table>
<h3 id="二、实验步骤"><a href="#二、实验步骤" class="headerlink" title="二、实验步骤"></a>二、实验步骤</h3><h4 id="第一步：数据标注"><a href="#第一步：数据标注" class="headerlink" title="第一步：数据标注"></a>第一步：数据标注</h4><ol>
<li><p>计划标注：</p>
<ul>
<li>人工标注验证集（大约4000条数据，需要20天左右，10月25日之前完成。）</li>
<li>思路：133个类别，每个类别标注30条数据。</li>
<li>方法：利用关键词筛选出每个类别对应的数据。</li>
</ul>
</li>
<li><p>实际标注：</p>
<ul>
<li>实际只标注了2000条数据</li>
<li>1500条有对应的类别，500条没有作为反例，共涉及80个类别。</li>
</ul>
</li>
</ol>
<h4 id="第二步：训练词向量"><a href="#第二步：训练词向量" class="headerlink" title="第二步：训练词向量"></a>第二步：训练词向量</h4><ul>
<li>方法：Word2vec</li>
<li>具体做法：<ol>
<li>下载最新的中文维基百科数据集（1.5g)</li>
<li>进行一系列预处理操作：<ul>
<li><a href="https://github.com/attardi/wikiextractor" target="_blank" rel="noopener">wikiextractor</a>解压，提取压缩包文本信息 wiki_00</li>
<li>繁转简：opencc-&gt; wiki.zh.txt</li>
<li>分句+数据清洗（只保留数字、英文和汉字）：fen_ju.py-&gt; new_sentence.txt</li>
<li>是否分词: jieba </li>
<li>是否去停词：停词表</li>
<li>统计句子长度、词频、字频，生成字典</li>
</ul>
</li>
<li>分词后训练word2vec模型,生成2种词向量。<ul>
<li>分词+去停词+300维</li>
<li>未分词+去停词+300维</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="第三步：无监督学习的句向量生成算法"><a href="#第三步：无监督学习的句向量生成算法" class="headerlink" title="第三步：无监督学习的句向量生成算法"></a>第三步：无监督学习的句向量生成算法</h4><ol>
<li><p>在文本相似度任务中比较各种句向量生成算法的效果</p>
<ol>
<li>加权平均算法</li>
<li>Quick-Thoughts算法</li>
<li>将Transformer融入Quick-Thought算法中，观察其效果</li>
<li>比较分词与未分词的差别</li>
</ol>
</li>
<li><p>具体做法</p>
<ul>
<li>实际数据共有1982条进行预测，预定义语句5166条，因为部分数据预处理后为空</li>
<li>本文修改了f1的计算方式—因为数据中存在没有对应的预定义操作的句子</li>
<li>对于深度学习模型，本文的实验环境有限，只选择了80万条训练样本</li>
</ul>
<ol>
<li><p>方法一：基于词向量的简单平均算法. 共耗时1771.08s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.6846</td>
<td align="center">0.7070</td>
<td align="center">0.6956</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.6868</td>
<td align="center">0.7070</td>
<td align="center">0.6968</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.6931</td>
<td align="center">0.7070</td>
<td align="center">0.7000</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.8070</td>
<td align="center">0.7050</td>
<td align="center">0.7526</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.9525</td>
<td align="center">0.6167</td>
<td align="center">0.7487</td>
</tr>
</tbody></table>
</li>
<li><p>方法二：基于字向量的简单平均算法，共耗时1762.93s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.6970</td>
<td align="center">0.7492</td>
<td align="center">0.7221</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.6996</td>
<td align="center">0.7492</td>
<td align="center">0.7235</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.7703</td>
<td align="center">0.7492</td>
<td align="center">0.7596</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.8749</td>
<td align="center">0.7391</td>
<td align="center">0.8013</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.9607</td>
<td align="center">0.5719</td>
<td align="center">0.7170</td>
</tr>
</tbody></table>
</li>
<li><p>方法三：基于词向量的加权平均算法. 共耗时1834.97s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.687219</td>
<td align="center">0.715719</td>
<td align="center">0.701180</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.687219</td>
<td align="center">0.715719</td>
<td align="center">0.701180</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.701180</td>
<td align="center">0.715719</td>
<td align="center">0.708375</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.838095</td>
<td align="center">0.706355</td>
<td align="center">0.766606</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.923699</td>
<td align="center">0.534448</td>
<td align="center">0.677119</td>
</tr>
</tbody></table>
</li>
<li><p>方法四：基于字向量的加权平均算法. 共耗时1787.23s.</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.692551</td>
<td align="center">0.733779</td>
<td align="center">0.712569</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.692988</td>
<td align="center">0.733779</td>
<td align="center">0.712801</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.754301</td>
<td align="center">0.733110</td>
<td align="center">0.743555</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.879508</td>
<td align="center">0.717726</td>
<td align="center">0.790424</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.949233</td>
<td align="center">0.537793</td>
<td align="center">0.686593</td>
</tr>
</tbody></table>
</li>
<li><p>方法五：基于字向量和词向量的加权平均算法. 共耗时1762.93s.<br><br>生成的句向量是600维</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.697328</td>
<td align="center">0.750502</td>
<td align="center">0.722938</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.698195</td>
<td align="center">0.750502</td>
<td align="center">0.723404</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.763624</td>
<td align="center">0.749833</td>
<td align="center">0.756666</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.890433</td>
<td align="center">0.728428</td>
<td align="center">0.801325</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.950423</td>
<td align="center">0.525753</td>
<td align="center">0.677003</td>
</tr>
</tbody></table>
</li>
<li><p>方法六：基于词向量的Quick_Thoughts算法</p>
<ol>
<li><p>生成训练样本Tfrecords文件—preprocess_dataset.py</p>
</li>
<li><p>训练模型—train.py</p>
<table>
<thead>
<tr>
<th align="center">需要调节的参数</th>
<th align="center">解释说明</th>
<th align="center">示例</th>
</tr>
</thead>
<tbody><tr>
<td align="center">word2vec_path</td>
<td align="center">word2vec文件的目录</td>
<td align="center">../data/sent_word_n/</td>
</tr>
<tr>
<td align="center">output_dir</td>
<td align="center">生成句向量的目录</td>
<td align="center">../output/sent_char_n/</td>
</tr>
<tr>
<td align="center">input_file_pattern</td>
<td align="center">tfrecord文件的命名格式</td>
<td align="center">../output/sent_word_n/train-?????-of-00010</td>
</tr>
<tr>
<td align="center">train_dir</td>
<td align="center">模型文件的保存位置</td>
<td align="center">..model/train/sent_char_n</td>
</tr>
</tbody></table>
</li>
<li><p>预测：生成句向量—predict.py</p>
</li>
<li><p>训练结果</p>
<ul>
<li><p>生成句向量消耗的时间：14.37s（小爱数据，1982条），24.17s（预定义数据集，5166条）</p>
</li>
<li><p>整个预测过程共消耗21950.48s</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.655587</td>
<td align="center">0.620067</td>
<td align="center">0.637332</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.655587</td>
<td align="center">0.620067</td>
<td align="center">0.637332</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.655587</td>
<td align="center">0.620067</td>
<td align="center">0.637332</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.661670</td>
<td align="center">0.620067</td>
<td align="center">0.640193</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.717054</td>
<td align="center">0.618729</td>
<td align="center">0.664273</td>
</tr>
<tr>
<td align="center">0.94</td>
<td align="center">0.847909</td>
<td align="center">0.596656</td>
<td align="center">0.700432</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
</li>
<li><p>方法七：基于字向量的Quick-Thoughts算法</p>
<ul>
<li><p>训练结果：</p>
<ul>
<li><p>生成句向量消耗的时间：<br>   13.72s（小爱数据，1982条），23.98s（预定义数据集，5166条）</p>
</li>
<li><p>共消耗21926.511472 s</p>
<table>
<thead>
<tr>
<th align="center">阈值</th>
<th align="center">P</th>
<th align="center">R</th>
<th align="center">F1值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.5</td>
<td align="center">0.673374</td>
<td align="center">0.671572</td>
<td align="center">0.672472</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.673374</td>
<td align="center">0.671572</td>
<td align="center">0.672472</td>
</tr>
<tr>
<td align="center">0.7</td>
<td align="center">0.673826</td>
<td align="center">0.671572</td>
<td align="center">0.672697</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">0.679756</td>
<td align="center">0.671572</td>
<td align="center">0.675639</td>
</tr>
<tr>
<td align="center">0.9</td>
<td align="center">0.839361</td>
<td align="center">0.667559</td>
<td align="center">0.743666</td>
</tr>
<tr>
<td align="center">0.92</td>
<td align="center">0.900742</td>
<td align="center">0.649498</td>
<td align="center">0.754761</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ul>
</li>
<li><p>方法八：融合Transformer的Quick-Thoughts算法（分词, 2700维）</p>
<ol>
<li><p>Transform的编码器得到的是一个[seq_length, dim]的向量，<br>因此探索Transformer编码器生成句向量的处理方式，然后与Quick-Thoughts融合</p>
<ul>
<li><p>生成句向量消耗的时间：26.67s（小爱数据，1982条），40.14s（预定义数据集，5166条）</p>
</li>
<li><p>模型参数设置（调整后得到的最优参数）：</p>
<table>
<thead>
<tr>
<th align="center">参数</th>
<th align="center">取值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">num_head</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">learing_rate</td>
<td align="center">0.001</td>
</tr>
<tr>
<td align="center">dim</td>
<td align="center">2700</td>
</tr>
<tr>
<td align="center">dropout_rate</td>
<td align="center">0.3</td>
</tr>
<tr>
<td align="center">batch_zie</td>
<td align="center">128</td>
</tr>
<tr>
<td align="center">num_ceng</td>
<td align="center">2</td>
</tr>
</tbody></table>
</li>
<li><p>预测结果对比：</p>
<table>
<thead>
<tr>
<th align="center">处理方式</th>
<th align="center">F1值（最优）</th>
<th align="center">预测消耗总时间（s）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">简单平均</td>
<td align="center">0.63</td>
<td align="center">26122.49</td>
</tr>
<tr>
<td align="center">直接求和</td>
<td align="center">0.33</td>
<td align="center">24774.29</td>
</tr>
<tr>
<td align="center">标准化后平均</td>
<td align="center">0.40</td>
<td align="center">24528.78</td>
</tr>
<tr>
<td align="center">标准化后求和</td>
<td align="center">0.39</td>
<td align="center">27671.44</td>
</tr>
<tr>
<td align="center">对简单平均后的向量进行标准化</td>
<td align="center">0.45</td>
<td align="center">24164.85</td>
</tr>
<tr>
<td align="center">对直接求和后的向量进行标准化</td>
<td align="center">0.1</td>
<td align="center">21877.37</td>
</tr>
</tbody></table>
<p>最优的F1值为0.69，阈值为0.999588。</p>
</li>
</ul>
<p><strong>结论：对于Transformer模型生成的词向量进行简单平均效果最好</strong></p>
</li>
</ol>
</li>
<li><p>方法九：融合Transformer的Quick-Thoughts算法（未分词，2700维）</p>
<ul>
<li><p>基于字向量的Transformer模型：</p>
<ol>
<li>损失函数值：1102.33</li>
<li>训练时间： 48704.05</li>
</ol>
</li>
<li><p><em>结果：最优的F1值为0.75，阈值为0.999442。*</em></p>
</li>
</ul>
</li>
<li><p>方法十：Transformer编码器+简单平均</p>
<table>
<thead>
<tr>
<th align="center">模型</th>
<th align="center">阈值</th>
<th align="center">最优的F1值</th>
<th align="center">预测时间</th>
<th align="center">日志文件名</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于字向量的Transformer模型</td>
<td align="center">0.999991</td>
<td align="center">0.41</td>
<td align="center">8686.1</td>
<td align="center">tr_char</td>
</tr>
<tr>
<td align="center">基于词向量的Transformer模型</td>
<td align="center">0.999999</td>
<td align="center">0.30</td>
<td align="center">9753.86</td>
<td align="center">tr_word</td>
</tr>
</tbody></table>
<p> <strong>结论：仅利用Transformer编码器无法揭示句子之间的相似程度。</strong></p>
</li>
</ol>
</li>
</ol>
<h4 id="第四步：训练细节"><a href="#第四步：训练细节" class="headerlink" title="第四步：训练细节"></a>第四步：训练细节</h4><p>  <strong>利用sent_word_rem数据集对Transformer进行调参</strong></p>
<ol>
<li><p><strong>第一次调参时模型存在部分错误：</strong></p>
<ol>
<li>mask应该相乘而不是相加</li>
<li>多头注意力的输出应该添加一个线性连接层</li>
<li>多头注意力层和全连接层没有添加drpout</li>
<li>词向量矩阵应该随机正态化，没有对词向量矩阵和切分后的q进行归一化</li>
</ol>
</li>
<li><p>第一调参得到的部分错误结果    </p>
<ul>
<li><p>（1）观察预先训练的词向量对模型结果的影响</p>
<table>
<thead>
<tr>
<th align="center">词向量表示方式</th>
<th align="center">损失函数值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">预先训练</td>
<td align="center">1257.96</td>
</tr>
<tr>
<td align="center">随机初始化</td>
<td align="center">1224.76</td>
</tr>
</tbody></table>
</li>
<li><p>（2）编码器和解码器层数对模型结果的影响</p>
<table>
<thead>
<tr>
<th align="center">层数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center"></td>
<td align="center">batch_size=128</td>
</tr>
</tbody></table>
</li>
<li><p>（3）编码维度对模型的结果影响</p>
<table>
<thead>
<tr>
<th align="center">维度</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">300</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
</tbody></table>
</li>
<li><p>（4）头数对模型结果的影响(双层，batch_size=128)</p>
<table>
<thead>
<tr>
<th align="center">头数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1225.45</td>
<td align="center">67956.94</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1225.45</td>
<td align="center">68619.52</td>
</tr>
</tbody></table>
</li>
<li><p>（5）batch_size对模型的影响(双层，num_head=4))</p>
<table>
<thead>
<tr>
<th align="center">batch_size</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">128</td>
<td align="center">1224.76</td>
<td align="center">63801.97</td>
</tr>
<tr>
<td align="center">64</td>
<td align="center">608.16</td>
<td align="center">68647.59</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p><strong>第二次调参</strong></p>
<ul>
<li><p>batch_size =128, ceng_shu=2, num_head=6</p>
</li>
<li><p>（1）batch_size对模型的影响(双层，num_head=4))</p>
<table>
<thead>
<tr>
<th align="center">batch_size</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">512</td>
<td align="center">OOM</td>
<td align="center">OOM</td>
</tr>
<tr>
<td align="center">256</td>
<td align="center">2457.10</td>
<td align="center">70999.85</td>
</tr>
<tr>
<td align="center">128</td>
<td align="center">1242.77</td>
<td align="center">71327.49</td>
</tr>
<tr>
<td align="center">64</td>
<td align="center">623.93</td>
<td align="center">71453.79</td>
</tr>
</tbody></table>
<p> <strong>问题</strong></p>
<pre><code>1. 为什么batch_size一般选择2的幂次？
   &lt;br&gt;因为GPU对2的幂次的batch可以发挥更佳的性能
2. batch_size对模型效果的影响？
   （1）batch_size过大，训练消耗的时间会缩短，但是模型容易陷入局部最优点。
      因为样本方差较小，可能会呆在一个局部最优点不动。
   （2）batch_size过小，模型同样不易收敛，损失函数容易震荡</code></pre></li>
<li><p>（2）编码器和解码器层数对模型结果的影响（batch_size=128,num_head=2)</p>
<table>
<thead>
<tr>
<th align="center">层数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1143.48</td>
<td align="center">71261.14</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">1222.36</td>
<td align="center">83205.03</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1224.50</td>
<td align="center">94817.31</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">1225.24</td>
<td align="center">106313.79</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1229.86</td>
<td align="center">117888.51</td>
</tr>
</tbody></table>
</li>
<li><p>（3）编码维度对模型的结果影响</p>
<pre><code>batch_size=128,num_head=8,
num_units=[512,2048],num_ceng=2
dim = 200时，训练结果不稳定，参考loss.png(16,17)
dim = 300时，num_head = 2</code></pre><table>
<thead>
<tr>
<th align="center">维度</th>
<th align="center">损失函数值</th>
<th align="left">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">200</td>
<td align="center">1143.52</td>
<td align="left">53102.00</td>
</tr>
<tr>
<td align="center">256</td>
<td align="center">1143.49</td>
<td align="left">63260.60</td>
</tr>
<tr>
<td align="center">300</td>
<td align="center">1143.48</td>
<td align="left">71201.66</td>
</tr>
<tr>
<td align="center">400</td>
<td align="center">1143.48</td>
<td align="left">94607.44</td>
</tr>
<tr>
<td align="center">512</td>
<td align="center">1143.47</td>
<td align="left">123626.50</td>
</tr>
</tbody></table>
</li>
<li><p>（4）头数对模型结果的影响(双层，batch_size=128)</p>
<table>
<thead>
<tr>
<th align="center">头数</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">2</td>
<td align="center">1143.48</td>
<td align="center">71261.14</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1227.60</td>
<td align="center">71728.84</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">1143.78</td>
<td align="center">71856.85</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1143.48</td>
<td align="center">71636.31</td>
</tr>
<tr>
<td align="center">10</td>
<td align="center">1143.48</td>
<td align="center">73083.46</td>
</tr>
<tr>
<td align="center">20</td>
<td align="center">1143.48</td>
<td align="center">74627.01</td>
</tr>
</tbody></table>
<p><strong>问题</strong>  </p>
<pre><code>1. 为什么头数为4的时候损失函数值比较高？</code></pre></li>
<li><p>（5）学习率对模型结果的影响（双层, num_head = 6, batch_size=128)</p>
<table>
<thead>
<tr>
<th align="center">rate</th>
<th align="center">损失函数值</th>
<th align="center">F1值（最优）</th>
<th align="center">阈值</th>
<th align="center">训练模型消耗时间（s)</th>
<th align="center">预测消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.00005</td>
<td align="center">1235.80</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">120375.26</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.000275</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.0005</td>
<td align="center">1143.48</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">71636.31</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.001</td>
<td align="center">1143.48</td>
<td align="center">0.69</td>
<td align="center">0.999588</td>
<td align="center">62813.25</td>
<td align="center">9296.24</td>
</tr>
<tr>
<td align="center">0.003</td>
<td align="center">1143.47</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">72178.98</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.005</td>
<td align="center">1143.47</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">71888.02</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">0.01</td>
<td align="center">1143.80</td>
<td align="center">0.69</td>
<td align="center">0.989927</td>
<td align="center">67776.89</td>
<td align="center">9477.49</td>
</tr>
</tbody></table>
</li>
<li><p>（6）探索直接输入预训练的词向量还是随机初始化效果较好<br>   （num_head =2, dim=300, num_units=[1200, 300], loss_19.png)</p>
<ul>
<li><p>开始主观设定阈值的取值范围是0.5：1：0.01</p>
<table>
<thead>
<tr>
<th align="center">是否采用预训练的词向量作为输入</th>
<th align="center">损失函数值</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">是</td>
<td align="center">1143.51</td>
<td align="center">62143.24</td>
</tr>
<tr>
<td align="center">否</td>
<td align="center">1143.48</td>
<td align="center">71201.66</td>
</tr>
</tbody></table>
</li>
<li><p>然后调整阈值取值范围为最小值：最大值：（最大值-最小值）/20</p>
<table>
<thead>
<tr>
<th align="center">是否采用预训练的词向量作为输入</th>
<th align="center">F1值（最优）</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">是</td>
<td align="center">0.63</td>
<td align="center">26122.49</td>
</tr>
<tr>
<td align="center">否</td>
<td align="center">0.63</td>
<td align="center">21611.44</td>
</tr>
</tbody></table>
</li>
</ul>
<p><strong>结论</strong>：利用预训练得到的词向量得到的输入，模型训练过程中会发生震荡，<br>  但是最终的结果与随机初始化相差不大，而且利用预先训练的词向量训练模型消耗的时间较少。</p>
</li>
<li><p>(7) 探索dropout的影响（随机初始化词向量）</p>
<pre><code>dim =300, num_head =2, num_units = [1200, 300],
num_ceng =2, batch_size = 128, 
learning_rate = 0.0005
阈值是用（最大值-最小值）/20为步长挑选出来的</code></pre><table>
<thead>
<tr>
<th align="center">rate</th>
<th align="center">损失函数值</th>
<th align="center">F1值（最优）</th>
<th align="center">阈值</th>
<th align="center">训练模型消耗时间（s)</th>
<th align="center">预测消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.1</td>
<td align="center">1143.47</td>
<td align="center">0.69</td>
<td align="center">0.999607</td>
<td align="center">62943.26</td>
<td align="center">9107.41</td>
</tr>
<tr>
<td align="center">0.3</td>
<td align="center">1143.48</td>
<td align="center">0.69</td>
<td align="center">0.999605</td>
<td align="center">62623.19</td>
<td align="center">9271.86</td>
</tr>
<tr>
<td align="center">0.5</td>
<td align="center">1143.58</td>
<td align="center">0.69</td>
<td align="center">0.999606</td>
<td align="center">62186.54</td>
<td align="center">9137.37</td>
</tr>
<tr>
<td align="center">0.8</td>
<td align="center">1144.84</td>
<td align="center">0.69</td>
<td align="center">0.999590</td>
<td align="center">63128.25</td>
<td align="center">9588.84</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
</ol>
<p>  <strong>调参结果</strong></p>
<pre><code>1. dim =300, num_head =2, num_units = [1200, 300],
   num_ceng =2, batch_size = 128, rate=0.3
2. learning_rate = 0.0005, 梯度更新公式在论文的基础上乘以learning_rate效果更好
3. 是否采用预训练的词向量对损失函数值影响不大（已有论文证明，而且本文的结果也证明是否采用预训练的词向量影响不大），但是采用预训练的词向量消耗的时间较短。
   在预测时对比两种模型在真是数据集中的效果，发现效果差别不大。
4. 受硬件限制无法训练一个与原论文相同的6层512维的模型，大概需要三天左右才能训练结束。</code></pre><h4 id="第五步：实验结果"><a href="#第五步：实验结果" class="headerlink" title="第五步：实验结果"></a>第五步：实验结果</h4><ol>
<li><p>实验环境:<strong>2核8g服务器</strong></p>
</li>
<li><p>生成处理后的数据集需要的时间</p>
<table>
<thead>
<tr>
<th align="center">数据集</th>
<th align="center">生成字向量或者词向量(s)</th>
<th align="center">获取词频文件(s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sent_char_n.txt</td>
<td align="center">4065.48</td>
<td align="center">445</td>
</tr>
<tr>
<td align="center">sent_char_rem.txt</td>
<td align="center">2663.45</td>
<td align="center">334</td>
</tr>
<tr>
<td align="center">sent_word_n.txt</td>
<td align="center">4594.10</td>
<td align="center">443</td>
</tr>
<tr>
<td align="center">sent_word_rem.txt</td>
<td align="center">3359.56</td>
<td align="center">366</td>
</tr>
</tbody></table>
</li>
<li><p>Quick-thoughts训练过程统计</p>
<ul>
<li><p>第一次生成训练数据的时间</p>
<table>
<thead>
<tr>
<th align="center">数据集</th>
<th align="center">词或字的总数量</th>
<th align="center">词向量中字或词的数量</th>
<th align="center">统计词频(s)</th>
<th align="center">生成词典和词向量文件(s)</th>
<th align="center">样本中句子包含<br>最少的词数量</th>
<th align="center">样本中句子<br>包含最多的词数量</th>
<th align="center">训练时的词数量</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sent_char_n.txt</td>
<td align="center">19354</td>
<td align="center">12820</td>
<td align="center">111.06</td>
<td align="center">10.43</td>
<td align="center">2</td>
<td align="center">874</td>
<td align="center">12820</td>
</tr>
<tr>
<td align="center">sent_char_rem.txt</td>
<td align="center">18936</td>
<td align="center">12403</td>
<td align="center">78.99</td>
<td align="center">10.28</td>
<td align="center">2</td>
<td align="center">767</td>
<td align="center">12403</td>
</tr>
<tr>
<td align="center">sent_word_n.txt</td>
<td align="center">3198221</td>
<td align="center">617297</td>
<td align="center">117.14</td>
<td align="center">140.89</td>
<td align="center">2</td>
<td align="center">432</td>
<td align="center">20000</td>
</tr>
<tr>
<td align="center">sent_word_rem.txt</td>
<td align="center">3196599</td>
<td align="center">615709</td>
<td align="center">92.02</td>
<td align="center">141.54</td>
<td align="center">2</td>
<td align="center">366</td>
<td align="center">20000</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">数据集</th>
<th align="center">生成TF文件耗费时间（s)</th>
<th align="center">训练模型消耗时间（s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">sent_char_n.txt</td>
<td align="center">86.21</td>
<td align="center">29555.58</td>
</tr>
<tr>
<td align="center">sent_char_rem.txt</td>
<td align="center">76.69</td>
<td align="center">29743.32</td>
</tr>
<tr>
<td align="center">sent_word_n.txt</td>
<td align="center">70.86</td>
<td align="center">29528.91</td>
</tr>
<tr>
<td align="center">sent_word_rem.txt</td>
<td align="center">80.22</td>
<td align="center">29533.81</td>
</tr>
</tbody></table>
</li>
<li><p>第二次重新分句后生成训练数据的时间</p>
<table>
<thead>
<tr>
<th align="center">文件</th>
<th align="center">字或词的数目</th>
<th align="center">训练样本中长度<br>小于30的句子数量</th>
<th align="center">句子的最短长度</th>
<th align="center">句子的最大长度</th>
<th align="center">生成record文件<br>消耗的时间(s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">new_sent_char_rem.txt</td>
<td align="center">9591</td>
<td align="center">640195</td>
<td align="center">2</td>
<td align="center">997</td>
<td align="center">78.80</td>
</tr>
<tr>
<td align="center">new_sent_word_rem.txt</td>
<td align="center">20000</td>
<td align="center">938007</td>
<td align="center">2</td>
<td align="center">300</td>
<td align="center">72.19</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>第一次训练的最终结果：</p>
<ul>
<li><p>第一次利用Quick-Thoughts预测时，阈值是固定的（0.9)。</p>
</li>
<li><p>第一次利用融合模型预测的时候，阈值是从0.5~1，步长为0.01，所以预测时间较长。</p>
</li>
<li><p>第一次利用融合模型预测时，基于词向量的融合模型效果较差，因此没有预测基于字向量的模型。</p>
</li>
<li><p>第一次训练时部分日志文件丢失，无法获取准确的训练时间。</p>
<table>
<thead>
<tr>
<th align="center">模型</th>
<th align="center">阈值</th>
<th align="center">最优的F1值</th>
<th align="center">预测时间</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于词向量的简单平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.7526</td>
<td align="center">1771.08</td>
</tr>
<tr>
<td align="center">基于字向量的简单平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.8013</td>
<td align="center">1762.93</td>
</tr>
<tr>
<td align="center">基于词向量的加权平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.7666</td>
<td align="center">1834.97</td>
</tr>
<tr>
<td align="center">基于字向量的加权平均（去停词）</td>
<td align="center">0.8</td>
<td align="center">0.7904</td>
<td align="center">1787.23</td>
</tr>
<tr>
<td align="center">基于字向量和词向量的加权平均</td>
<td align="center">0.8</td>
<td align="center">0.8013</td>
<td align="center">1762.93</td>
</tr>
<tr>
<td align="center">基于词向量的Quick-Thought Vectors算法</td>
<td align="center">0.9</td>
<td align="center">0.743666</td>
<td align="center">2253.14</td>
</tr>
<tr>
<td align="center">基于字向量的Quick-Thought Vectors算法</td>
<td align="center">0.9</td>
<td align="center">0.664273</td>
<td align="center">2250.50</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought Vectors算法（分词+去停词）</td>
<td align="center">0.8</td>
<td align="center">0.396015</td>
<td align="center">24528.78</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought Vectors算法（不分词+去停词）</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>第二次重新分句后的最终结果</p>
<ul>
<li><p>对于简单平均算法和加权平均算法没有重新生成词向量，进行训练。</p>
</li>
<li><p>重新对数据进行清洗，删除维基百科中部分无意义的信息，<br>并且按照中文习惯重新进行分句。</p>
</li>
<li><p>训练模型模型时，全都随机初始化词向量。（所以Quick-thoughts效果较差）</p>
<p><strong>实验结果</strong></p>
<table>
<thead>
<tr>
<th align="center">模型</th>
<th align="center">阈值</th>
<th align="center">最优的F1值</th>
<th align="center">训练时间（s)</th>
<th align="center">预测时间(s)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">基于字向量的Quick-Thoughts模型</td>
<td align="center">0.922823</td>
<td align="center">0.601799</td>
<td align="center">31274.17(8.7h)</td>
<td align="center">9354.07</td>
</tr>
<tr>
<td align="center">基于词向量的Quick-Thoughts模型</td>
<td align="center">0.885814</td>
<td align="center">0.647385</td>
<td align="center">31443.50(8.7h)</td>
<td align="center">9192.68</td>
</tr>
<tr>
<td align="center">基于字向量的Transformer模型</td>
<td align="center">0.999912</td>
<td align="center">0.725537</td>
<td align="center">47057.17(13h)</td>
<td align="center">8854.41</td>
</tr>
<tr>
<td align="center">基于词向量的Transformer模型</td>
<td align="center">0.999942</td>
<td align="center">0.575241</td>
<td align="center">72569.82(20h)</td>
<td align="center">8830.47</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought模型（未分词）</td>
<td align="center">0.999464</td>
<td align="center">0.63471</td>
<td align="center">31274.17(8.7h)+47057.17(13h)</td>
<td align="center">9397.37</td>
</tr>
<tr>
<td align="center">融合Transformer的Quick-Thought模型（分词）</td>
<td align="center">0.999164</td>
<td align="center">0.64511</td>
<td align="center">31443.50(8.7h)+72569.82(20h)</td>
<td align="center">9328.97</td>
</tr>
</tbody></table>
</li>
<li><p>结论：</p>
<ol>
<li>两个编码器全都随机初始化词向量后，Quick-Thoughts的性能下降。<br>并且基于字向量的模型比基于词向量的模型效果较差。</li>
<li>仅用Transformer编码器，基于字向量的模型效果较好，基于词向量的模型效果特别差。</li>
<li>融合后的模型无论是基于词向量还是字向量，与融合前的Quick-thoughts算法相比，没有显著的提升。</li>
<li>融合前后的Quick-thoughts算法，基于字向量的模型效果都比基于词向量的模型效果差，可能是因为Quick-thoughts算法没有<br>输入预先训练的词向量，还有融合后句向量的维度变为2700，对模型的结果也有一定的影响，但是由于计算条件的限制无法对<br>句向量的维度进行探索。</li>
</ol>
</li>
</ul>
</li>
</ol>
<h3 id="三、其它"><a href="#三、其它" class="headerlink" title="三、其它"></a>三、其它</h3><ul>
<li><p>已解决的问题</p>
<ol>
<li>词向量文件太大, 无法加载(已解决)—利用np.load加载.npy文件，直接就是numpy数组</li>
<li>为什么字输入比词输入消耗时间少？—原因：字的维度较小，权重矩阵较小。</li>
<li>换新电脑后经常出现无法打开GitHub的官网情况，原因为本地的DNS无法进行解析，<br>可以修改C:\Windows\System32\drivers\etc\hosts文件，具体细节参考<br><a href="https://blog.csdn.net/believe_s/article/details/81539747" target="_blank" rel="noopener">连不上GitHub的解决方案</a></li>
<li>label smoothing 标签平滑</li>
<li>batch_size固定的太死，无法预测非batch_size的东西</li>
<li>预测时的batch必须要与训练时的batch保持一样吗？—不一定</li>
<li>不足一个batch的数据在预测时是如何处理的？— tf.shape可以获取变量的维度信息，就算是维度为None</li>
<li>transformer的输入是等长的还是不等长的？<ul>
<li>编码器-解码器的每个batch的长度不一样，每个batch填充到这个batch内的最大长度。</li>
</ul>
</li>
<li>Quick-thoughts算法不需要对句子进行padding，transformer的每个batch需要进行padding</li>
<li>os.remove只能删除文件，shutil.rmtree可以删除指定的目录</li>
<li>第一次生成的vocab.txt中含有特殊字符，需要重新生成。<br>之前的vocab.txt是利用词向量文件重新生成的，所以要想生成新的必须首先生成词向量。<br>第二次修改直接选择词频较高的词语作为字典，总数量不超过20000。</li>
<li>重新生成训练样本文件，深度学习模型的词向量嵌入均随机初始化。</li>
</ol>
</li>
<li><p>未解决的问题</p>
<ol>
<li><p>长度和重合度数据集未标注完成，共2000条,并且标注后的样本去停词后会变成空值</p>
</li>
<li><p>受硬件限制，无法探索词向量的维度和句向量的维度对模型效果的影响</p>
</li>
<li><p>未登录词如何处理？<br>  <br>目前采取的方法：随机初始化，用0进行padding<br>  <br>（1）加unnk,索引为1<br>  <br>（2）随机初始化一个向量<br>  <br>（3）padding 和未登录词的区别</p>
</li>
<li><p>quick-thoughts 效果较差</p>
<ul>
<li>基于词向量的F1值0.7，基于字向量的F1值0.75.</li>
</ul>
</li>
<li><p>loss一直保持不变，是什么原因？</p>
<ul>
<li>猜测原因：<ol>
<li>说明参数一直都没有得到更新</li>
<li>没有添加dropout，为啥dropout可以防止过拟合？dropout相当于集成</li>
<li>batch_size过大，网络会收敛到局部最优点；<br>batchz_size太小，类别较多时，loss可能会一直震荡</li>
<li>学习率过大，transformer的学习率在论文中有对应的公式</li>
</ol>
</li>
<li>真实原因：<ol>
<li>按照论文中的学习率公式修改学习率后，损失函数缓慢下降，但是又开始震荡<ul>
<li>论文中的学习率公式如何得到的？？？</li>
<li>为什么按照论文中的公式表示学习率就会缓慢下降？？？？<br>理论上学习率如果足够小，肯定可以收敛。、</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>beam search在预测时会遇到，本文参考tensor2tensor的代码，并没有看懂    </p>
<ul>
<li>主要函数：<ol>
<li>grow_alive </li>
<li>grow_finished </li>
<li>grow_topk </li>
<li>inner_loop</li>
<li>is_not_finished</li>
</ol>
</li>
<li>不懂的点<ol>
<li>tf.bitcast()具体是怎末实现的</li>
<li>长度惩罚项及结束搜索的条件</li>
<li>主要函数的功能</li>
</ol>
</li>
</ul>
</li>
<li><p>机器翻译时，解码器的第一个输入是<code>&lt;s&gt;</code>表示开头，直到<code>&lt;e&gt;</code>结束</p>
</li>
<li><p>对词层面上的优化（感觉意义不大）：</p>
<ol>
<li>对句子中所有的词向量取最大值</li>
<li>对句子中所有的词向量取最小值</li>
<li>对句子中所有的词向量取平均值<br>生成句向量：<ol>
<li>进行拼接</li>
<li>删除掉最大值和最小值,然后再进行平均</li>
</ol>
</li>
</ol>
</li>
<li><p>第二次重新分句后没有训练词向量和字向量文件，quick-thoughts算法只能全部随机初始化，效果较差。</p>
</li>
</ol>
</li>
</ul>
<h3 id="四、-参考链接"><a href="#四、-参考链接" class="headerlink" title="四、 参考链接"></a>四、 参考链接</h3><ol>
<li><a href="https://blog.csdn.net/CiciliarCai/article/details/52948275" target="_blank" rel="noopener">维基百科</a></li>
<li><a href="https://github.com/PrincetonML/SIF" target="_blank" rel="noopener">加权平均算法</a></li>
<li><a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Transformer</a></li>
<li><a href="https://github.com/jinjiajia/Quick_Thought" target="_blank" rel="noopener">Quick-thoughts算法</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/" data-id="ck9mlp1t90004oot9b9ooakoj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8F%A5%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/" rel="tag">句向量在文本相似度上的应用研究</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-data-structure" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/21/data-structure/" class="article-date">
  <time datetime="2020-03-21T00:51:52.000Z" itemprop="datePublished">2020-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/21/data-structure/">data_structure</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、位运算总结"><a href="#一、位运算总结" class="headerlink" title="一、位运算总结"></a>一、位运算总结</h1><h2 id="1-常用的位运算符"><a href="#1-常用的位运算符" class="headerlink" title="1.常用的位运算符"></a>1.常用的位运算符</h2><table>
<thead>
<tr>
<th align="center">运算符</th>
<th align="center">功能</th>
</tr>
</thead>
<tbody><tr>
<td align="center">与：&amp;</td>
<td align="center">相同为1，不同为0</td>
</tr>
<tr>
<td align="center">或：&#124;</td>
<td align="center">只要有一个为1就为1</td>
</tr>
<tr>
<td align="center">异或：^</td>
<td align="center">相同为0，不同为1</td>
</tr>
<tr>
<td align="center">左移：&lt;&lt;</td>
<td align="center">二进制位左移，低位补0，相当于乘以2的幂</td>
</tr>
<tr>
<td align="center">右移：&gt;&gt;</td>
<td align="center">二进制位右移，高位补0，相当于除以2的幂</td>
</tr>
<tr>
<td align="center">取反：~</td>
<td align="center">每个位置取相反的，比如1变为0，0变为1</td>
</tr>
</tbody></table>
<h2 id="2-常用的性质"><a href="#2-常用的性质" class="headerlink" title="2.常用的性质"></a>2.常用的性质</h2><ol>
<li>任意数与1进行与运算，得到的是该数的最后一个二进制位</li>
<li>异或运算 0 ^ 任意数 = 0, a ^ a =0, a ^ b ^ b=a</li>
<li>左移相当于乘以2的幂,右移相当于除以2的幂</li>
<li>正数的最高位为0,补码和反码都与原码相同</li>
<li>负数的最高位为1,反码的符号位与原码相同,数值位相反;补码是反码加1</li>
<li>计算机中正数用原码表示，负数用补码表示</li>
<li>~(a)=-(a+1)</li>
<li>对正数取反后，得到的是补码(负数用补码表示0)，需要减1得到反码；然后对数值位取反</li>
<li>对负数取反后，得到的是正数，正数的原码补码反码都一样</li>
<li>将正数减1后取反可以得到对应的负数，即~(a-1)= -a</li>
<li>位运算是原码之间进行的，因此a &amp; (~(1&lt;&lt;31)) = a, 可以用来防止数值溢出</li>
</ol>
<h1 id="二、图论"><a href="#二、图论" class="headerlink" title="二、图论"></a>二、图论</h1><h2 id="1-深度优先遍历—递归"><a href="#1-深度优先遍历—递归" class="headerlink" title="1.深度优先遍历—递归"></a>1.深度优先遍历—递归</h2><h2 id="2-宽度优先遍历–队列"><a href="#2-宽度优先遍历–队列" class="headerlink" title="2.宽度优先遍历–队列"></a>2.宽度优先遍历–队列</h2><h2 id="3-拓扑排序"><a href="#3-拓扑排序" class="headerlink" title="3.拓扑排序"></a>3.拓扑排序</h2><p>   基本概念：把有向无环图转化为一个线性序列，得到的序列可能不唯一。<br>   如果点A出现在点B前面，则一定不存在点B到点A的路径</p>
<h2 id="4-连通块"><a href="#4-连通块" class="headerlink" title="4.连通块"></a>4.连通块</h2><h1 id="递归—与动态规划，链表，-图论这些都有重合"><a href="#递归—与动态规划，链表，-图论这些都有重合" class="headerlink" title="递归—与动态规划，链表， 图论这些都有重合"></a>递归—与动态规划，链表， 图论这些都有重合</h1><ul>
<li>非递归空间：返回地址、参数和局部变量。可能会出现堆栈溢出</li>
<li>递归空间: 定义的全局变量</li>
<li>尾递归： return f(x_1) + f(x_2)   c和c++支持尾递归优化，Java和python不支持尾递归优化</li>
</ul>
<p>经典例题：</p>
<ul>
<li>反转字符串</li>
<li>杨辉三角</li>
<li>反转链表</li>
<li>斐波拉数列</li>
<li>爬楼梯</li>
<li><strong>不同的二叉搜索树</strong>：<br>   <br>1.题目描述：寻找1,2,…n生成的不同的二叉搜索树，并按照层次遍历输出<br>   <br>2.思路：假设i为根节点,则左子树是由1,2,…i-1组成的,右子树是由i+1,i+2,…n组成，即满足卡特兰数<br>   <br>3.生成的二叉搜索树的个数就是卡特兰数，h(n)表示第n+1项，h(0)=h(1)=1,<br>   <br>  即h(n) = h(0)h(n) + h(1)h(n-1) + h(2)h(n-2) + …. + h(n)h(0)<br>   <br>4.代码实现(generate_trees(start, end))：<ol>
<li>遍历所有的节点作为根节点</li>
<li>递归寻找各个节点的左右子树  </li>
<li>遍历左右子树,将左右子树与根节点连接起来</li>
<li>迭代终止条件：start &gt; end ,返回[None,]</li>
</ol>
</li>
</ul>
<h1 id="三、链表"><a href="#三、链表" class="headerlink" title="三、链表"></a>三、链表</h1><h2 id="1-反转链表"><a href="#1-反转链表" class="headerlink" title="1.反转链表"></a>1.反转链表</h2><h2 id="2-删除链表中的节点"><a href="#2-删除链表中的节点" class="headerlink" title="2.删除链表中的节点"></a>2.删除链表中的节点</h2><h2 id="3-合并链表"><a href="#3-合并链表" class="headerlink" title="3.合并链表"></a>3.合并链表</h2><h2 id="4-循环列表"><a href="#4-循环列表" class="headerlink" title="4.循环列表"></a>4.循环列表</h2><h1 id="四、数组–查询"><a href="#四、数组–查询" class="headerlink" title="四、数组–查询"></a>四、数组–查询</h1><h2 id="1-二分查找法"><a href="#1-二分查找法" class="headerlink" title="1.二分查找法"></a>1.二分查找法</h2><p>##</p>
<h1 id="五、字符串"><a href="#五、字符串" class="headerlink" title="五、字符串"></a>五、字符串</h1><h2 id="1-最长公共前缀"><a href="#1-最长公共前缀" class="headerlink" title="1.最长公共前缀"></a>1.最长公共前缀</h2><h1 id="六、队列和栈"><a href="#六、队列和栈" class="headerlink" title="六、队列和栈"></a>六、队列和栈</h1><h1 id="七、二叉树"><a href="#七、二叉树" class="headerlink" title="七、二叉树"></a>七、二叉树</h1><h1 id="八、排序算法"><a href="#八、排序算法" class="headerlink" title="八、排序算法"></a>八、排序算法</h1><h1 id="九、动态规划"><a href="#九、动态规划" class="headerlink" title="九、动态规划"></a>九、动态规划</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/21/data-structure/" data-id="ck9mlp1t50001oot9cpxndgm1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%A2%86%E6%89%A3%E5%88%B7%E9%A2%98%EF%BC%8C-%E4%BB%A3%E7%A0%81-https-gitee-com-chen-liu-123-project/" rel="tag">领扣刷题，[代码](https://gitee.com/chen_liu_123/project)</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-git常用命令" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/10/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="article-date">
  <time datetime="2020-03-10T01:56:57.000Z" itemprop="datePublished">2020-03-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/10/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">git常用命令总结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>git学习对git有了相对系统的认识，将常用命令进行梳理总结<br><br>1. 如何在本地创建一个git库并与远程仓库联系起来？<br><br>2. 如何同时绑定多个远程仓库？<br><br>3. 查看日志，版本回退<br><br>4. 分支管理等</li>
</ul>
<h3 id="git常用命令："><a href="#git常用命令：" class="headerlink" title="git常用命令："></a>git常用命令：</h3><ol>
<li><p>git diff: 查看修改的文件内容</p>
</li>
<li><p><strong>git reflog: 记录每次执行的git命令</strong></p>
</li>
<li><p>git log: 查看提交历史</p>
</li>
<li><p>撤销修改</p>
<ul>
<li>如果在本地(工作区)：<br><strong>git  checkout –file_name:<br>  撤销工作区的修改，即恢复被删除的文件<br>  如果没有–，表示切换分支</strong></li>
<li>如果已经commit(暂存区)：<ol>
<li>git reset HEAD file_name: 把暂存区的修改撤销，版本回退的前提是没有提交到远程仓库，</li>
<li>然后再git checkout –file_name</li>
</ol>
</li>
</ul>
</li>
<li><p>删除文件</p>
<ul>
<li>rm file_name 删除本地文件</li>
<li>git rm file_name 删除版本库中的文件，并且git commit</li>
<li>删除远程仓库中的文件<ol>
<li>rm file_name</li>
<li>git commit -m “remove file_name”</li>
<li>git push origin master</li>
</ol>
</li>
<li>删除远程仓库？？？？？？</li>
</ul>
</li>
<li><p>本地初始化git仓库，添加到远程仓库：<br> git remote add origin <a href="mailto:git@github.com">git@github.com</a>:michaelliao/learngit.git</p>
<ul>
<li>git remote rm origin 删除本地仓库与远程的连接</li>
<li>git push -u: 因为远程仓库为空，第一次push必须要添加-u</li>
</ul>
</li>
<li><p>分支管理</p>
<ul>
<li>git branch name创建分支</li>
<li>git checkout name 切换分支</li>
<li>git checkout -b name  或git switch -c name 创建并切换分支</li>
<li>git branch 查看分支，前面带*表示当前分支</li>
<li>git merge 合并分支</li>
<li>git branch -d name 删除分支</li>
<li>git brach -D name 强行删除分支</li>
<li>git merge发生冲突后，查看冲突文件，并修改对应的内容</li>
<li>git log –graph 显示分支合并图</li>
<li>git log –pretty=oneline –abbrev-commit 更清晰显示图</li>
</ul>
</li>
<li><p>实际工作中一般不会在master分支工作，设置分支，在分支上进行工作，然后合并</p>
<ul>
<li>修复bug时通常重新建立分支，再分支上修复后再合并，然后删除bug分支</li>
<li>git pull 拉取远程仓库的代码，强制覆盖本地的代码，<br>防止本地修改丢失，应该使用git stash保存本地的修改</li>
<li>git stash 缓存未完成的工作</li>
<li>git stash list 查看缓存的内容</li>
<li>git stash pop 恢复未完成的内容，并删除stash</li>
<li>如果master分支有bug, 其余分支也有相同的bug, 可以利用git cherry-pick id 进行修复，id是指commit的id<br>远程仓库的默认名字origin</li>
<li>git push origin master 把本地分支的所有内容推送到远程分支<br>查看远程库信息，使用git remote -v；</li>
</ul>
</li>
<li><p>本地新建的分支如果不推送到远程，对其他人就是不可见的；</p>
<ul>
<li>从本地推送分支，使用git push origin branch-name，</li>
<li>如果推送失败，先用git pull origin master抓取远程的新提交；</li>
<li>在本地创建和远程分支对应的分支 git checkout -b branch-name origin/branch-name，<br>本地和远程分支的名称最好一致；</li>
<li>git pull 失败后提示，可以建立本地分支和远程分支的关联 git branch –set-upstream branch-name origin/branch-name</li>
<li>从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。</li>
<li>git rebase 使分支提交历史成为一条直线，更加清晰</li>
</ul>
</li>
<li><p>标签，id一般较长，将版本号与id绑在一起更方便</p>
<ul>
<li>git tag name commit_id 创建版本号</li>
<li>git tag -a name -m 描述信息 commit_id 添加描述信息</li>
<li>git show name 展示绑定的commit_id信息</li>
<li>git log 查看标签列表</li>
<li>删除标签 git tag -d name<br><br>上传到远程仓库 git push origin name<br><br>全部上传 git push origin –tags<br><br>如果上传到远程，需要先删除本地，再删除远程的，删除<br>远程的代码 git push origin :refs/tags/tagname</li>
</ul>
</li>
<li><p>本地仓库可以同时关联多个远程分支</p>
<ul>
<li>git init 初始化本地目录为git仓库</li>
<li>git remote add github <a href="mailto:git@github.com">git@github.com</a>/chen_liu_123/learngit.git </li>
<li>git remote add origin  <a href="http://gitee.com/chen_liu_123/learngit.git" target="_blank" rel="noopener">http://gitee.com/chen_liu_123/learngit.git</a> </li>
<li>git push github master</li>
<li>git push origin master</li>
</ul>
</li>
<li><p>配置git</p>
<ul>
<li>给命令设置别名 git config –global alias.ci commit</li>
<li>global表示为所有的仓库执行相同的命令</li>
</ul>
</li>
<li><p>强制合并 git pull origin master –allow-unrelated-histories</p>
</li>
</ol>
<ul>
<li>参考网站：<a href="https://www.liaoxuefeng.com/wiki/896043488029600" target="_blank" rel="noopener">学习网站</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/10/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" data-id="ck9mlp1t80003oot97khpa0ko" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/" rel="tag">git常用命令学习</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-first-blog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/09/first-blog/" class="article-date">
  <time datetime="2020-03-09T03:04:30.000Z" itemprop="datePublished">2020-03-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/03/09/first-blog/">first_blog</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h1><h2 id="环境：-Win10"><a href="#环境：-Win10" class="headerlink" title="环境： Win10"></a>环境： Win10</h2><h2 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h2><ol>
<li>下载Git,直接去官网下载<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">地址</a></li>
<li>下载Node.js，同样在官网上下载<a href="https://nodejs.org/zh-cn/download/" target="_blank" rel="noopener">地址</a></li>
<li>启动Git,在命令行中执行以下命令:<code>npm install -g hexo-cli</code>下载Hexo</li>
<li>找到自己希望放置博客的文件夹下， 执行<code>Hexo init</code></li>
<li>执行<code>Hexo generate</code>, <code>Hexo deploy</code>,<code>Hexo server</code>, 在浏览器中输入<code>localhost:4000</code>就可以查看。</li>
<li>要想让别人也看得见，必须将Hexo部署到GitHub上。创建GitHub仓库，名字为user_name.github.io</li>
<li>利用<code>git remote add origin 远程仓库的地址</code>将本地的仓库添加到远程仓库中</li>
<li>修改本地目录下的文件_comfig.yml，在最后添加： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type: git</span><br><span class="line">repository: https:&#x2F;&#x2F;github.com&#x2F;user_name&#x2F;user_name.github.io.git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure></li>
<li>然后将目录下的文件上传到远程仓库中：<code>git push -u origin master</code></li>
<li>执行以下指令完成部署：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure></li>
<li>添加新文章步骤：<ol>
<li>hexo new <titile> ：   在source/_posts目录下创建md文件</li>
<li>hexo clean：           清除缓存文件 (db.json) 和已生成的静态文件 (public)。</li>
<li>hexo generate：        生成静态文件</li>
<li>hexo deploy：          部署到GitHub上</li>
</ol>
</li>
</ol>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>   部署过程十分简单，如果报错百度就可以解决，本文主要参考了以下文章：</p>
<ol>
<li><a href="https://www.imooc.com/wenda/detail/402086" target="_blank" rel="noopener">hexo init命令时，出现错误，该怎么办？</a></li>
<li><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo官方文档</a></li>
<li><a href="https://www.jianshu.com/p/390f202c5b0e" target="_blank" rel="noopener">5分钟搞定个人博客-hexo</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/09/first-blog/" data-id="ck9mlp1t10000oot9cux5g5og" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%EF%BC%9F-%E5%88%A9%E7%94%A8GitHub%E5%92%8CHexo%E6%9E%84%E5%BB%BA/" rel="tag">如何部署自己的博客系统？--利用GitHub和Hexo构建</a></li></ul>

    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/" rel="tag">git常用命令学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%A5%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/" rel="tag">句向量在文本相似度上的应用研究</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%EF%BC%9F-%E5%88%A9%E7%94%A8GitHub%E5%92%8CHexo%E6%9E%84%E5%BB%BA/" rel="tag">如何部署自己的博客系统？--利用GitHub和Hexo构建</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%A2%86%E6%89%A3%E5%88%B7%E9%A2%98%EF%BC%8C-%E4%BB%A3%E7%A0%81-https-gitee-com-chen-liu-123-project/" rel="tag">领扣刷题，[代码](https://gitee.com/chen_liu_123/project)</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">git常用命令学习</a> <a href="/tags/%E5%8F%A5%E5%90%91%E9%87%8F%E5%9C%A8%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8%E7%A0%94%E7%A9%B6/" style="font-size: 10px;">句向量在文本相似度上的应用研究</a> <a href="/tags/%E5%A6%82%E4%BD%95%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%EF%BC%9F-%E5%88%A9%E7%94%A8GitHub%E5%92%8CHexo%E6%9E%84%E5%BB%BA/" style="font-size: 10px;">如何部署自己的博客系统？--利用GitHub和Hexo构建</a> <a href="/tags/%E9%A2%86%E6%89%A3%E5%88%B7%E9%A2%98%EF%BC%8C-%E4%BB%A3%E7%A0%81-https-gitee-com-chen-liu-123-project/" style="font-size: 10px;">领扣刷题，[代码](https://gitee.com/chen_liu_123/project)</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/04/30/%E4%B8%AD%E6%96%87%E5%8F%A5%E5%90%91%E9%87%8F%E7%A0%94%E7%A9%B6/">中文句向量研究</a>
          </li>
        
          <li>
            <a href="/2020/03/21/data-structure/">data_structure</a>
          </li>
        
          <li>
            <a href="/2020/03/10/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">git常用命令总结</a>
          </li>
        
          <li>
            <a href="/2020/03/09/first-blog/">first_blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Chen Liu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>